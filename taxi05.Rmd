---
title: "Желтое такси в Нью-Йорке"
output:
  html_document: 
    toc: yes
  html_notebook: default
---

*Задача этого проекта — научиться предсказывать количество поездок в ближайшие часы в каждом районе Нью-Йорка. Для того, чтобы её решить, сырые данные необходимо агрегировать по часам и районам. Агрегированные данные будут представлять собой почасовые временные ряды с количествами поездок из каждого района. Похожие задачи возникают на практике, если вам необходимо спрогнозировать продажи большого количества товаров в большом количестве магазинов, объём снятия денег в сети банкоматов, посещаемость разных страниц сайта и т.д.*

# Неделя 5: Прогнозирование с помощью регрессии {#week05}

### 0. Вступление и подготовка к работе {#w5s0}

Авторы проекта предложили использовать инструмент на выбор. Т.к. для меня это второй проект, то я решил его сделать на R. Ввиду того, что в специализации используется Python, я постарался снабдить код подробными комментариями, чтобы (при желании) было легче разобраться.

Ссылки на прошлые недели: [Неделя 1](https://beta.rstudioconnect.com/content/2211/taxi01.html), [Неделя 2](https://beta.rstudioconnect.com/content/2218/taxi02.html), [Неделя 3](https://beta.rstudioconnect.com/content/2298/taxi03.html). [Неделя 4](https://beta.rstudioconnect.com/content/2343/taxi04.html)

Для работы потребуются следующие библиотеки и файлы:

```{r libs, message=FALSE}
library(wesanderson) # палитры для графиков
library(scales) # в помощь графикам
library(ggplot2) # графики
library(magrittr) # c'est ne pas une pipe
library(lubridate) # работа с датами
library(data.table) # работа с таблицами данных
library(assertthat) # проверки
library(glmnet)

source('helpers03.R') # здесь вспомогательные функции
source('helpers04.R') # здесь вспомогательные функции
source('helpers05.R') # здесь вспомогательные функции

# Sys.setlocale('LC_ALL','utf-8') # если неверно отображается кириллица
```

Информация о версиях библиотек и системе:


```{r versions}
sessionInfo()
```


### 1. Выберите признаки {#w5s1}

*Для каждой из шести задач прогнозирования $\hat{y}_{T|T+i}, i=1,\dots,6$ сформируйте выборки. Откликом будет $y_{T+i}, i=1,\dots,6$ при всевозможных значениях $T$, а признаки можно использовать следующие:*

- идентификатор географической зоны — категориальный

- год, месяц, день месяца, день недели, час — эти признаки можно пробовать брать и категориальными, и непрерывными, можно даже и так, и так

- синусы, косинусы и тренды, которые вы использовали внутри регрессионной компоненты ARIMA

- сами значения прогнозов ARIMA $\hat{y}_{T|T+i}^{ARIMA}$

- количество поездок из рассматриваемого района в моменты времени $y_{T}, y_{T-1},\dots,y_{T-K}$ (параметр $K$ можно подбирать; попробуйте начать, например, с 6)

- количество поездок из рассматриваемого района в моменты времени $y_{T-24}, y_{T-48},\dots,y_{T-24*Kd}$ (параметр $Kd$ можно подбирать; попробуйте начать, например, с 2)

- суммарное количество поездок из рассматриваемого района за предшествующие полдня, сутки, неделю, месяц

*Будьте внимательны при создании признаков — все факторы должны быть рассчитаны без использования информации из будущего: при прогнозировании  $\hat{y}_{T|T+i}, i=1,\dots,6$ вы можете учитывать только значения y до момента времени T включительно.*




Я попробую построить модель на основе данных с 2013 года, поэтому я не смогу использовать модели ARIMA из прошлой недели, где данные были взяты всего за месяц (а из-за большого количества регрессоров модель очень долго обучалась), поэтому здесь я буду использовать лаги с 1 по 6, а также скользящие суммы за 6, 12, 24 и 168 часов и в дополнение к ним в качестве регрессоров по 49 синусов и косинусов (недельных и годовых), тренд, год, месяц и день недели как категориальные переменные, а также их попарные взаимодействия. 

На этой неделе мне хотелось бы попробовать линейную модель с $L_1$ и $L_2$ регуляризацией -- эластичной сетью из пакета `glmnet`. Эта регуляризация [задается](https://cran.r-project.org/web/packages/glmnet/vignettes/glmnet_beta.html#lin) двумя параметрами -- $\lambda$ и $\alpha$, где первый собственно сам коэффициент регуляризации, а второй отвечает за доли "гребневого" и лассо- регуляризаторов. 

К сожалению, я не смогу использовать 6 моделей с моими признаками. Если строить одну модель (на каждый час прогноза), то количество наблюдений за 3.5 года составит: 29178 x 102 = 3е+9. У каждого наблюдения будет 11 лагов и скользящих сумм, 196 синусов-косинусов, тренд, по 23 признака часа дня, 6 признаков дня недели 11 признаков месяца и 3 признака года. К ним добавятся `r 6*23` взаимодействий день-час, `r 6*11` взаимодействий день-месяц, `r 23*11` взаимодействия час-месяц, что получается уже более 450 столбцов. При все при этом еще нужны взаимодействия регион-час, регион-день и здесь счет уже идет на тысячи. По самой оптимистичной прикидки 3e+9 столбцов по 2е+3 признаков, где каждое число представляется 48 байтами (класс *numeric*), что составит 3е+14 байт или порядка 300 000 Гб. Поэтому я принял решение настроить модель для каждого региона отдельно, всего получится 612 моделей. Этот подход тоже сопряжен с долгими вычислениями: гиперпараментры для каждой модели будут подбираться 3-4 минуты, значит уйдет как минимум 40 часов (!) машинного времени на обучение.

Подготовим данные, синусы/косинусы, тренд и данные из даты:

```{r prepare_date, cache=TRUE}
# прогноз на 1-6 часов
h <- 6

# используемые регионы
regions_102 <- c(1075, 1076, 1077, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 
                 1132, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 
                 1181, 1182, 1183, 1184, 1221, 1222, 1223, 1224, 1225, 1227, 
                 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1272, 1273, 
                 1274, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 
                 1287, 1326, 1327, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 
                 1338, 1339, 1376, 1377, 1378, 1380, 1382, 1383, 1384, 1385, 
                 1386, 1387, 1388, 1389, 1390, 1426, 1431, 1434, 1435, 1436, 
                 1437, 1438, 1439, 1441, 1442, 1480, 1482, 1483, 1530, 1532, 
                 1533, 1580, 1630, 1684, 1733, 1734, 1783, 2068, 2069, 2118, 
                 2119, 2168)

# загрузим данные по 102 регионам и преобразуем время
data_full <- fread('data_out/regions_102_2013-2016.csv')
data_full <- data_full[, date := parse_date_time(date, "YmdHMS")]


# синусы-косинусы и тренд
n_xreg <- 49
xreg <- get_xreg(n_xreg, 1, nrow(data_full), years = TRUE, trend = TRUE)

# признаки времени: категории года, месяцев, дней недели и часа дня
data_time <- data_full[,1]
data_time[, `:=`(year = year(date),
                 month = factor(month(date)),
                 wday = factor(wday(date)),
                 hour = factor(hour(date)))]
```

Остальные данные (лаги, скользящие суммы и целевые переменыые) я буду формировать прямо перед обучением.


### 2. Подготовьте данные{#w5s2}

Разбейте каждую из шести выборок на три части:

- обучающая, на которой будут настраиваться параметры моделей — всё до апреля 2016

- тестовая, на которой вы будете подбирать значения гиперпараметров — май 2016

- итоговая, которая не будет использоваться при настройке моделей вообще — июнь 2016

Я буду пользоваться одной большой таблицей для обучения и одной таблицей для целевых переменных. Необходимые выборки я буду получать индексами:

```{r split_dataset, cache=TRUE}
# индексы

# обучающая выборка
idx_train <- data_full[date <= ymd_h('2016-04-30 17'), which = TRUE]
# тестовая
idx_test <- data_full[date >= ymd_h('2016-04-30 23') & 
                          date <= ymd_h('2016-05-31 17'), 
                      which = TRUE]
# для проверки июня
idx_june <- data_full[date >= ymd_h('2016-05-31 23') & 
                          date <= ymd_h('2016-06-30 17'), 
                      which = TRUE]
```


### 3. Выберите вашу любимую регрессионную модель и настройте её{#w5s3}

*Выберите вашу любимую регрессионную модель и настройте её на каждом из шести наборов данных, подбирая гиперпараметры на мае 2016. Желательно, чтобы модель:*

- допускала попарные взаимодействия между признаками

- была устойчивой к избыточному количеству признаков (например, использовала регуляризаторы)

Чтобы подобрать модель `glmnet` сначала необходим зафиксировать $\alpha$ и задать значения перебора $\lambda4$. Вот так выглядит зависимость среднего отклонения в зависимости от $\lambda$ для случайно выбранного региона:

```{r plot, fig.align='center', cache=TRUE}
# выберем регион
reg <- 61

# тренировочная выборка и целевая переменная
data <- cbind(data_time, xreg)
X <- model.matrix(~ . -date , data)
Y <- data_full[,-1] %>% as.matrix
y <- Y[,reg]
# y <- (data_full[,-1] %>% as.matrix)[,reg]

# регуляризатор лямбда, альфу возьмем = 0.2
lambdas <- 10**seq(-5, 5, length.out = 50)

# рассчитаем модель
fit <- glmnet(X[idx_train,], y[idx_train], alpha = 0.2, lambda = lambdas)

# построим график
maes <- get_maes(fit)
my_pal <- wes_palette(name = "Moonrise2")
plot_maes(maes) + 
    ggtitle(paste('MAE на обучающей и тестовой выборках региона', regions_102[reg]))
```

[Подберем](#find_alpha_lambda_h) гиперпараметры $\alpha$ и $\lambda$ для каждого региона:

```{r models_fit, eval=FALSE}
# итератор
is <- seq_along(regions_102)

for (i in is){
    cat(regions_102[i])
    
    # здесь я считаю лаги и суммы для региона, эта таблица для всех прогнозов
    lags_n_sums <- make_lags_and_sums(data_full[,-1][, ..i])[,-1]
    
    # собираем предикторы в одну таблицу 
    data <- cbind(data_time, xreg, lags_n_sums)
    
    # переводим в формат матрицы и вводим взаимодействия
    X <- model.matrix(~ . + wday:hour + hour:month -date , data)
    
    # здесь целевые переменные на 1-6 часов
    new_y <- data_full[,-1][,..i][
        , shift(.SD, 1:h, type = 'lead'), .SDcols = 1]
    
    # для каждого региона настраиваем h моделей
    for (j in 1:h) {
        # подбираем гиперпараметры, считаем ошибки и возвращаем модель
        best <- find_alpha_lambda_h(j)
        # сохраняем модель
        saveRDS(list(best$model, cbind(reg = regions_102[i], best$info)),
                paste0('models/glm_1_', regions_102[i], '_', j, '.rds'))
        cat('.')
    }
    cat(fill = TRUE)
}
```




### 4. Постройте прогнозы для тестовой выборки{#w5s4}

*Выбранными моделями постройте для каждой географической зоны и каждого конца истории от 2016.04.30 23:00 до 2016.05.31 17:00 прогнозы на 6 часов вперёд; посчитайте в ноутбуке ошибку прогноза согласно функционалу Q, определённому на прошлой неделе, уменьшилась по сравнению с той, которую вы получили методом индивидуального применения моделей ARIMA. Если этого не произошло, попробуйте улучшить ваши модели.*


```{r read_may, cache=TRUE}
files <- list.files('models/', pattern = "\\.rds$")

models <- list() # список для моделей
tbl_info <- list() # список для таблиц с информацией о них

# аккуратно, в папке должно быть ровно 612 моделей :(
for (i in seq_along(files)){
    f <- readRDS(paste0('models/', files[i]))
    reg <- paste0('r', substr(files[i], 7, 10))
    j <- substr(files[i], 12, 12) %>% as.integer
    models[[reg]][[j]] <- f[[1]]
    tbl_info[[i]] <- f[[2]]
}

# составляем табличку с информацией
tbl_info <- rbindlist(tbl_info)
```

Информация по моделям уже посчитана (на основании результата на тестовой выборке и подбирались гиперпараметры), поэтому можно вывести среднюю ошибку по всей выборке и по горизонту прогноза:

```{r tbl_info_mae}
tbl_info[, .(mae_train = mean(train), mae_test = mean(test))]
tbl_info[, .(mae_train = mean(train), mae_test = mean(test)), by = h]
```

Проверим результат, предварительно [рассчитав](#get_pred_table_h) таблицу с прогнозами:

```{r mae_may, cache=TRUE}
tbl_pred <- get_pred_table_h(models, idx_test, 6) 
get_Q(tbl_pred)
```

Среднее отклонение уменьшилось с 50 до 18.8. Эта способ сработал гораздо лучше, чем на предыдущей неделе.

### 5. Постройте прогнозы на июнь 2016{#w5s5}

*Итоговыми моделями постройте прогнозы для каждого конца истории от 2016.05.31 23:00 до 2016.06.30 17:00 и запишите все результаты в один файл в формате geoID, histEndDay, histEndHour, step, y. Здесь geoID — идентификатор зоны, histEndDay — день конца истории в формате id,y, где столбец id состоит из склеенных через подчёркивание идентификатора географической зоны, даты конца истории, часа конца истории и номера отсчёта, на который делается предсказание (1-6); столбец y — ваш прогноз.*

Правильным было бы переобучить модели на обобщенной выборке (трейн + тест), когда были подобраны оптимальные гиперпараметры, но из-за невнимательности сохраненные модели были обучены лишь на тренировочной выборке, а переобучение заняло бы достаточно времени. Из-за того, что данных довольно много, можно ожидать, что результаты не будут сильно отличаться от получившихся. Поэтому просто рассчитаем результат и отправим на kaggle:

```{r mae_june, cache=TRUE}
tbl_pred <- get_pred_table_h(models, idx_june, 6) 
get_Q(tbl_pred)
```

Если бы шла борьба за медали, немного улучшить метрику можно было бы заменив отрицательные значения нулями. Более того, по тестовой выборке можно было бы подобрать порог и значение замены, чтобы еще немного улучшить метрику. 

```{r mae_june_cor, cache=TRUE}
# сделаем замену на 0 и пересчитаем ошибку
# tbl_pred[y <= 0, y := 0]
# get_Q(tbl_pred)
# 17.90735

# сколько значений меньше 0?
tbl_pred[y <= 0, .N]
```

Но здесь это не имеет большого смысла, я оставлю предыдущее решение.

### 6. Загрузите полученный файл на kaggle {#w5s6}

![Мой второй сабмит](data_out/my_3d.png)


### 7. Опубликуйте ноутбук (+ выводы) {#w5s7}

Ноутбук опубликован на [rstudioconnect.com](https://beta.rstudioconnect.com/content/2364/taxi05.html). Файлы проекта также можно найти и на [гитхабе](https://github.com/yurkai/taxi).

Эта подход оказался точнее предыдущего. Тем не менее, он оказался не очень удачным -- точность все еще не велика. Самым главным недостатком является долгое время обучения (>40 часов машинного времени), поэтому нужно очень тщательно подходить к выбору признаков. В моем случае я выбрал не самые лучшие из них (например, забыл включить лаги больше 6 часов, нет дневных лагов, но было уже поздно начинать заново); вероятно, не все взаимодействия и фурье-компоненты (особенно годовые!) действительно нужны. Из 641 признака для каждого наблюдения в моделях оставлось по 350-400. К сожалению, у меня была только одна попытка, и если бы лидерборд был важным показателем, то ее можно было бы признать успешной. Тем не менее, на следующей неделе я опять попробую найти более точное решение (и, надеюсь, менее трудоемкое).

### 8. Приложение: код функций {#w5s8}

##### 8.1 mae

```{r}
# считаем ошибку mae
mae <- function(y, pred){
    abs(y - pred) %>% mean
}
```


##### 8.2 get_maes_h

```{r}
# возвращает mae для каждого значения lambda на train/test
# fit -- модель glmnet
# new_y - таблица с будущим
get_maes_h <- function(fit, h){
    
    # считаем целевую переменную для каждой лямбды
    lambdas <- fit$lambda
    train_pred <- predict(fit, newx=X[idx_train,], s = fit$lambdas) %>% 
        data.table
    test_pred <- predict(fit, newx=X[idx_test,], s = fit$lambdas) %>% 
        data.table
    
    # считаем ошибки на трейне/тесте
    mae_train <- sapply(train_pred, function(y_pred) 
        mae(new_y[[h]][idx_train], y_pred)) %>% as.numeric
    mae_test <- sapply(test_pred, function(y_pred) 
        mae(new_y[[h]][idx_test], y_pred)) %>% as.numeric
    
    # возвращаем лямбды, ошики на трейне/тесте
    data.table(lambda = lambdas, train = mae_train, test = mae_test)
}
```


##### 8.3 find_alpha_lambda_h


```{r}
# подбираем альфу и лямбду для модели с h
find_alpha_lambda_h <- function(h,
    alphas =seq(0, 1, by = 0.1), 
    lambdas = 10**seq(-5, 5, length.out = 50)){
    
    # табличка с данными
    maes <- data.table()
    
    # перебор ведем по каждому возможному значению альфы
    for (alpha in alphas){
        # фитим модель
        fit <- glmnet(X[idx_train,], new_y[[h]][idx_train], 
                      alpha = alpha, lambda = lambdas)
        # считаем ее ошибки для всех лямбд
        maes <- rbindlist(list(maes, cbind(alpha = alpha, get_maes_h(fit, h))))
    }
    
    # считаем модель с оптимальным гиперпараметрами
    fit <- glmnet(X[idx_train,], new_y[[h]][idx_train], 
                  alpha = maes[test == min(test)][1, alpha], 
                  lambda = maes[test == min(test)][1, lambda])
    
    # возвращаем модель и ее гиперпараметры со значениями ошибок
    list(info = cbind(h = h, maes[test == min(test)][1]), model = fit)
}
```


##### 8.4 get_pred_table_h

```{r}
# строим прогноз по моделям glmnet
# data_full -- глобальная
# n_xreg -- глобальная (как и на обучении)
# xreg -- глобальная (как и на обучении)
# data_time -- глобальная (как и на обучении)
get_pred_table_h <- function(models, idx, h = 6){
    # итераторы
    is <- seq_along(regions_102)

    tbl_pred <- list()
    
    for (i in is){
        cat(regions_102[i])
        
        # формируем лаги для каждого региона
        lags_n_sums <- make_lags_and_sums(data_full[,-1][, ..i])[,-1]
        data <- cbind(data_time, xreg, lags_n_sums)
        X <- model.matrix(~ . + wday:hour + hour:month -date , data)
        new_y <- data_full[,-1][,..i][
            , shift(.SD, 1:h, type = 'lead'), .SDcols = 1]
        
        # временный список с h таблицами региона
        tmp <- list()
        # прогноз на 1-h часов
        for (j in 1:h) {
            # достаем модель
            fit <- models[[paste0('r', regions_102[i])]][[j]]
            # прогноз
            pred <- predict(fit, newx=X[idx,], s = fit$lambda)[,1]
            # сохраняем прогноз в таблицу и добавляем id
            tmp[[j]] <- data.table(
                id = paste(regions_102[i], get_subm_ymdh(data[idx, date]), 
                           j, sep = '_'),
                y = pred)
        }
        # разворачиваем таблицу по региону и добавляем в список
        tbl_pred[[i]] <- rbindlist(tmp)
        
    }
    # разворачиваем таблицы регионов
    tbl_pred <- rbindlist(tbl_pred)
    
    tbl_pred
}
```







